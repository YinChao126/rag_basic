# ⚡ QuickStart：从零跑通第一个 RAG 系统

> 📚 **本章目标**：用最短的时间跑通一个 RAG 示例系统，**直观感受RAG相比纯LLM的巨大优势**。

> 💡 **核心价值**：通过实际对比，你会看到RAG如何让AI回答更**准确、可追溯、符合实际**。

------

## 一、环境准备

项目使用 [`uv`](https://docs.astral.sh/uv/) 管理依赖（比 pip 快 10 倍⚡）

### 1. 安装 uv

```bash
pip install uv
```



### 2. 克隆项目

```bash
git clone https://github.com/YinChao126/RAG-.git
```



### 3. VScode环境配置

请特别注意ROOT目录是**quickstart**

`cd RAG-/samples/quickstart`

用vscode打开并安装依赖

虚拟环境构建（可选但强烈建议）

```bash
uv venv
cd .venv/scripts
./activate
cd ../../
```

环境依赖同步，uv 会自动读取 `pyproject.toml` 并构建环境：

```bash
uv sync
```



------

## 二、示例场景简介

我们将构建一个最简单的 **企业知识问答系统**，通过对比展示RAG的优势：

### 📋 场景设置

- **知识库**：一份公司FAQ文档（`data/faq.md`）
- **模型**：调用 Dashscope（通义千问）接口
- **框架**：LangChain 1.x + Chroma 向量数据库

### 🎯 对比目标

我们将用**同一个问题**测试两种方式：

| 方式 | 特点 | 问题 |
|------|------|------|
| ❌ **纯LLM** | 只基于训练数据回答 | 回答可能不准确、无法追溯来源 |
| ✅ **RAG系统** | 基于知识库检索后回答 | 回答准确、可追溯、符合实际 |

### 🚀 RAG的核心优势

通过本示例，你将直观看到：

1. ✅ **准确性**：RAG基于知识库内容，回答更准确可靠
2. ✅ **可追溯性**：RAG可以标注信息来源，便于验证
3. ✅ **针对性**：RAG能回答知识库中的特定问题
4. ✅ **可控性**：通过更新知识库即可更新答案

------

## 三、项目结构

```text
quickstart/
├── data/
│   └── faq.md          # 示例知识库文件（公司FAQ）
├── main.py             # 🎯 一键对比脚本（强烈推荐！）
├── rag_basic.py        # RAG核心脚本
├── only_llm.py         # 纯LLM对比脚本
└── chroma_db/          # 向量数据库（首次运行自动创建）
```

> 💡 **提示**：
> - `main.py` 是最佳入口，可以一键对比两种方式的效果
> - `chroma_db/` 目录会在首次运行时自动创建，无需手动操作

------

## 四、知识库

已经创建文件 `data/faq.md`，内容如下：

```markdown
# 公司常见问题

## Q: 公司报销流程是怎样的？
A: 员工填写报销单并附上发票，经部门审批后财务统一打款。

## Q: 项目代码在哪里托管？
A: 所有项目代码托管在 GitHub 企业仓库，请使用公司邮箱注册访问。

## Q: 技术支持邮箱？
A: support@company.com
```

------

## 五、代码说明

### 5.1 纯LLM脚本（`only_llm.py`）

**功能**：直接调用LLM回答问题，不检索知识库

**特点**：
- 简单直接，只调用API
- 回答基于模型的训练数据
- 无法追溯信息来源

### 5.2 RAG脚本（`rag_basic.py`）

**功能**：基于知识库检索后回答问题

**核心流程**：
```
文档切片 → 向量化 → 存储 → 检索 → 生成答案
```

**特点**：
- ✅ 回答基于知识库内容
- ✅ 可以追溯信息来源
- ✅ 回答更准确可靠

### 5.3 对比脚本（`main.py`）

**功能**：一键对比两种方式的效果差异

**优势**：
- 🎯 直观展示RAG的优势
- 📊 自动对比关键指标
- 💡 帮助快速理解RAG价值



------

## 七、运行对比（核心环节）

### 🎯 方式1：一键对比（强烈推荐！）

使用 `main.py` 可以**同时对比**纯LLM和RAG的效果，直观看到差异：

```bash
uv run python main.py
```

**运行效果**：
1. 先展示纯LLM的回答（基于训练数据）
2. 再展示RAG的回答（基于知识库）
3. 最后自动对比两种方式的差异

**输出示例**：

```text
======================================================================
🚀 RAG vs LLM 对比演示
======================================================================

第一部分：纯LLM回答
----------------------------------------------------------------------
🤖 [方式1] 纯LLM回答（无知识库）
问题: 公司报销流程是怎样的？

⏱️  耗时: 2.35 秒

回答:
----------------------------------------------------------------------
公司报销流程可能因企业规模、行业和内部管理制度的不同而有所差异，
但一般来说，标准的报销流程通常包括以下几个步骤：

1. 发生费用
2. 收集并整理票据
3. 填写报销单
...（回答很长，但可能不符合企业实际情况）
----------------------------------------------------------------------

按 Enter 键继续查看RAG回答...

第二部分：RAG回答
----------------------------------------------------------------------
📚 [方式2] RAG回答（基于知识库）
问题: 公司报销流程是怎样的？

⏱️  耗时: 3.12 秒

回答:
----------------------------------------------------------------------
员工填写报销单并附上发票，经部门审批后财务统一打款。
----------------------------------------------------------------------

📚 引用来源:
  - data/faq.md 片段 1

======================================================================
📊 对比总结
======================================================================
对比项              纯LLM                    RAG系统
----------------------------------------------------------------------
回答长度            500+ 字符                30 字符
响应时间            2.35秒                   3.12秒
引用来源            无                       1个
======================================================================
💡 关键差异:
======================================================================
✅ RAG系统:
   - 回答基于知识库内容，更准确可靠
   - 可以追溯信息来源
   - 能够回答知识库中的特定问题

❌ 纯LLM:
   - 回答基于训练数据，可能不准确
   - 无法追溯信息来源
   - 无法回答知识库中的特定问题
======================================================================
```

---

### 📝 方式2：单独运行

如果想单独测试某个方式：

**测试纯LLM：**
```bash
uv run python only_llm.py
```

**测试RAG系统：**
```bash
uv run python rag_basic.py
```

---

### 🔍 RAG的核心优势总结

通过对比，你可以清晰看到RAG的**四大核心优势**：

| 优势 | 说明 | 实际价值 |
|------|------|---------|
| ✅ **准确性** | 基于知识库内容，而非训练数据 | 回答更可靠，符合实际情况 |
| ✅ **可追溯性** | 可以标注信息来源 | 便于验证和审计，增强可信度 |
| ✅ **针对性** | 能回答知识库中的特定问题 | 适合企业内部知识问答 |
| ✅ **可控性** | 通过更新知识库即可更新答案 | 无需重新训练模型，成本低 |

**一句话总结**：
> **纯LLM会说得"漂亮"，RAG让它说得"对"且"有据可查"。**

------

## 八、常见问题

### Q1: 首次运行需要做什么？

**A:** 只需要：
1. 设置环境变量 `QWEN_API_KEY`
2. 运行 `uv sync` 安装依赖
3. 运行 `python main.py` 即可

数据库会自动创建，无需手动操作。

### Q2: 如何修改测试问题？

**A:** 编辑 `main.py`，修改 `TEST_QUESTION` 变量：

```python
TEST_QUESTION = "你的问题"
```

### Q3: 如何添加更多知识？

**A:** 编辑 `data/faq.md`，添加更多问答对：

```markdown
## Q: 新问题？
A: 新答案
```

然后删除 `chroma_db` 目录，重新运行脚本即可自动重建数据库。

### Q4: 为什么RAG比LLM慢？

**A:** RAG需要额外的检索步骤，所以稍慢一些。但换来的是：
- ✅ 更准确的回答
- ✅ 可追溯的来源
- ✅ 符合实际情况

这个时间成本是值得的。

---

## 九、进一步学习

完成本示例后，建议继续学习：

- 📖 **第2-4章**：了解LLM和Prompt的基础概念
- 🔧 **第5-6章**：深入理解RAG的核心概念和工作流程  
- 📊 **第7-8章**：学习如何评估和优化RAG系统
- 🚀 **第10章**：了解RAG的发展趋势和学习路径

------

## 💬 本章总结

### 🎯 核心收获

通过本示例，你应该已经：

1. ✅ **跑通了第一个RAG系统** - 建立了感性认识
2. ✅ **看到了RAG的优势** - 准确性、可追溯性、针对性
3. ✅ **理解了RAG的价值** - 让AI回答更可靠、更实用

### 💡 关键洞察

> **"LLM 会说得漂亮，RAG 让它说得对。"**

**纯LLM的局限**：
- ❌ 回答基于训练数据，可能不准确
- ❌ 无法追溯信息来源
- ❌ 无法回答知识库中的特定问题

**RAG系统的优势**：
- ✅ 回答基于知识库，准确可靠
- ✅ 可以追溯信息来源
- ✅ 能回答知识库中的特定问题
- ✅ 通过更新知识库即可更新答案

### 🚀 下一步

现在你已经对RAG有了**感性认识**，接下来我们将：

1. **第2-4章**：了解LLM和Prompt的基础概念
2. **第5-6章**：深入理解RAG的核心概念和工作流程
3. **第7-8章**：学习如何评估和优化RAG系统

> 💡 **提示**：理解概念很重要，但不需要深入原理。重点是知道"是什么"和"怎么用"，而不是"为什么"。