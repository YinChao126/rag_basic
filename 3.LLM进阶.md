# LLMè¿›é˜¶ï¼šå‚æ•°è°ƒä¼˜ä¸è®°å¿†ç®¡ç†å®æˆ˜

> **ç›®çš„**ï¼šæœ¬ç¯‡é¢å‘å·²ç»æŒæ¡LLMåŸºç¡€çš„å¼€å‘è€…ï¼Œå¸®åŠ©ä½ çœŸæ­£â€œç”¨å¥½â€å¤§æ¨¡å‹ã€‚
>  æŒæ¡æ ¸å¿ƒå‚æ•°è°ƒä¼˜æ€è·¯ï¼Œç†è§£æ¨¡å‹çš„æ— è®°å¿†ç‰¹æ€§ï¼Œå¹¶å­¦ä¼šè®¾è®¡â€œä¸Šä¸‹æ–‡+è®°å¿†ç³»ç»Ÿâ€æ¥å¼¥è¡¥çŸ­æ¿ã€‚

------

## ä¸€ã€æ ¸å¿ƒå‚æ•°ä¸è°ƒä¼˜é€»è¾‘

åœ¨LLMè°ƒç”¨ä¸­ï¼Œå‚æ•°æ˜¯æ€§èƒ½ä¸æˆæœ¬çš„â€œçµé­‚â€ã€‚
 ç†è§£å¹¶æ­£ç¡®è°ƒä¼˜è¿™äº›å‚æ•°ï¼Œå¾€å¾€æ¯”æ›´æ¢æ¨¡å‹æ›´é‡è¦ã€‚

| å‚æ•°              | å«ä¹‰                | å½±å“æ–¹å‘                 |
| ----------------- | ------------------- | ------------------------ |
| **temperature**   | æ§åˆ¶è¾“å‡ºéšæœºæ€§      | è¶Šä½è¶Šç¨³å®šï¼Œè¶Šé«˜è¶Šæœ‰åˆ›æ„ |
| **top_p**         | æ§åˆ¶é‡‡æ ·æ¦‚ç‡èŒƒå›´    | é™åˆ¶å€™é€‰é›†åˆï¼Œæé«˜ä¸€è‡´æ€§ |
| **top_k**         | é™å®šæœ€é«˜æ¦‚ç‡çš„Kä¸ªè¯ | æé«˜ç”Ÿæˆè´¨é‡ä¸é€Ÿåº¦       |
| **max_tokens**    | è¾“å‡ºçš„æœ€å¤§tokenæ•°   | æ§åˆ¶æˆæœ¬ä¸å»¶è¿Ÿ           |
| **system prompt** | ç³»ç»Ÿè§’è‰²ä¸è¡Œä¸ºå®šä¹‰  | å†³å®šæ•´ä½“è¯­æ°”ä¸å®šä½       |

------

## äºŒã€æµå¼è¾“å‡ºä¸Tokenç›‘æ§

LLMç”Ÿæˆå†…å®¹çš„è¿‡ç¨‹æ˜¯â€œé€è¯é¢„æµ‹â€ã€‚
 é€šè¿‡**æµå¼è¾“å‡ºï¼ˆStreamingï¼‰**ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°å®æ—¶æ‰“å°æ•ˆæœï¼Œä¾¿äºç›‘æ§ç”Ÿæˆé€Ÿåº¦ä¸æ€§èƒ½ã€‚

### å®ç°æ•ˆæœ

- æ¨¡æ‹Ÿäººç±»æ‰“å­—èˆ¬è¾“å‡º
- ç»Ÿè®¡tokenç”Ÿæˆé€Ÿç‡
- è§‚å¯Ÿtemperatureå¯¹ç”Ÿæˆçš„å½±å“

### ç¤ºä¾‹ä»£ç 

```python
import os
import time
from openai import OpenAI
import json

class QwenStreamClient:
    def __init__(self, api_key=None, base_url=None, temperature=0.7):
        self.api_key = api_key or os.environ.get("QWEN_API_KEY")
        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.temperature = temperature
        
        if not self.api_key:
            raise ValueError("QWEN_API_KEY environment variable not set")
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    def stream_completion(self, prompt, model="qwen-plus", max_tokens=2048, temperature=None):
        """æµå¼è°ƒç”¨Qwenæ¨¡å‹å¹¶å®æ—¶æ‰“å°token"""
        if temperature is None:
            temperature = self.temperature
            
        print(f"\n=== å¼€å§‹ç”Ÿæˆ (temperature={temperature}) ===")
        print(f"è¾“å…¥: {prompt}")
        print("-" * 50)
        print("è¾“å‡º: ", end="", flush=True)
        
        try:
            start_time = time.time()
            full_response = ""
            token_count = 0
            
            # åˆ›å»ºæµå¼è¯·æ±‚
            stream = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # å¤„ç†æµå¼å“åº”
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    token = chunk.choices[0].delta.content
                    print(token, end="", flush=True)
                    full_response += token
                    token_count += 1
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"\n" + "-" * 50)
            print(f"ç”Ÿæˆå®Œæˆ!")
            print(f"æ€»tokenæ•°: {token_count}")
            print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"å¹³å‡é€Ÿåº¦: {token_count/elapsed_time:.1f} token/ç§’")
            
            return full_response
            
        except Exception as e:
            print(f"\né”™è¯¯: {e}")
            return None

def test_temperature_effects():
    """æµ‹è¯•ä¸åŒtemperatureå€¼å¯¹è¾“å‡ºçš„å½±å“"""
    client = QwenStreamClient()
    
    # æµ‹è¯•prompt
    test_prompts = [
        # "è¯·ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•",
        # "å†™ä¸€ä¸ªå…³äºæœºå™¨äººçš„çŸ­æ•…äº‹å¼€å¤´",
        "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
    ]
    
    temperatures = [0.1, 0.5, 0.9, 1.2]  # æµ‹è¯•ä¸åŒçš„temperatureå€¼
    
    for i, prompt in enumerate(test_prompts):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i+1}/{len(test_prompts)}")
        print(f"Prompt: {prompt}")
        print('='*60)
        
        for temp in temperatures:
            print(f"\n--- Temperature = {temp} ---")
            response = client.stream_completion(prompt, temperature=temp)
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼ï¼Œç”¨æˆ·å¯ä»¥è¾“å…¥ä¸åŒçš„promptå’Œtemperature"""
    client = QwenStreamClient()
    
    print("=== Qwenå¤§æ¨¡å‹äº¤äº’æµ‹è¯• ===")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("è¾“å…¥ 'temp X' è®¾ç½®temperature (ä¾‹å¦‚: temp 0.8)")
    print("å½“å‰temperature:", client.temperature)
    
    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥prompt: ").strip()
            
            if user_input.lower() == 'quit':
                break
            elif user_input.lower().startswith('temp '):
                try:
                    new_temp = float(user_input.split()[1])
                    client.temperature = new_temp
                    print(f"Temperatureè®¾ç½®ä¸º: {new_temp}")
                except (IndexError, ValueError):
                    print("æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: temp <æ•°å€¼>")
                continue
            elif not user_input:
                continue
                
            # è°ƒç”¨æ¨¡å‹
            response = client.stream_completion(user_input)
            
        except KeyboardInterrupt:
            print("\n\né€€å‡ºç¨‹åº")
            break
        except Exception as e:
            print(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1 - æ¸©åº¦å½±å“æµ‹è¯• (è‡ªåŠ¨æµ‹è¯•ä¸åŒtemperature)")
    print("2 - äº¤äº’æ¨¡å¼ (æ‰‹åŠ¨è¾“å…¥prompt)")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        test_temperature_effects()
    else:
        interactive_mode()
```

------

## ä¸‰ã€Temperatureè°ƒä¼˜æŒ‡å—

| åœºæ™¯                      | æ¨èTemperature | ç‰¹ç‚¹           |
| ------------------------- | --------------- | -------------- |
| **äº‹å®å‹é—®ç­” / ä»£ç ç”Ÿæˆ** | 0.1ï½0.3        | ç¨³å®šã€ç²¾ç¡®     |
| **é€šç”¨å†™ä½œ / æ€»ç»“ç¿»è¯‘**   | 0.5ï½0.7        | å¹³è¡¡åˆ›é€ ä¸ç¨³å®š |
| **åˆ›æ„å†™ä½œ / çµæ„Ÿè„‘æš´**   | 0.8ï½1.2        | å¤šæ ·ã€æœ‰åˆ›æ„   |

> ğŸ“ˆ æ¸©åº¦è¶Šé«˜ï¼Œæ¨¡å‹æ¢ç´¢çš„æ¦‚ç‡ç©ºé—´è¶Šå¤§ï¼›è¶Šä½ï¼Œåˆ™æ›´å€¾å‘äºé€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯ã€‚

------

## å››ã€Top-p ä¸ Top-k ç­–ç•¥

äºŒè€…å¸¸æ­é…ä½¿ç”¨ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼š

| å‚æ•°      | å«ä¹‰                  | æ¨èå€¼    | åº”ç”¨å»ºè®®               |
| --------- | --------------------- | --------- | ---------------------- |
| **Top-p** | ä¿ç•™ç´¯è®¡æ¦‚ç‡å‰pçš„å€™é€‰ | 0.8ï½0.95 | é™åˆ¶è¾“å‡ºåˆ†å¸ƒå°¾éƒ¨å™ªå£°   |
| **Top-k** | åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯ | 20ï½100   | é™ä½éšæœºæ€§ã€æé«˜ç¨³å®šæ€§ |

> å»ºè®®å…ˆè°ƒæ•´ **temperature**ï¼Œå†å¾®è°ƒ **top_p/top_k**ã€‚
>  Top-pè¶Šå°ï¼Œè¾“å‡ºæ›´é›†ä¸­ï¼›Top-kè¶Šå¤§ï¼Œè¾“å‡ºæ›´å‘æ•£ã€‚

------

## äº”ã€LLMæ²¡æœ‰â€œè®°å¿†â€ï¼šä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¸‹æ–‡

### åŸå› 

- LLM æ˜¯**æ— çŠ¶æ€è®¡ç®—æ¨¡å‹**ï¼Œæ¯æ¬¡ç”Ÿæˆéƒ½åªåŸºäºè¾“å…¥çš„ promptã€‚
- å®ƒæ²¡æœ‰â€œè®°å¿†â€ï¼Œä¸Šä¸‹æ–‡ä¸è¿ç»­ã€‚
- â€œContext Windowâ€ å†³å®šäº†æ¨¡å‹ä¸€æ¬¡èƒ½è®°ä½å¤šå°‘å†…å®¹ã€‚
   ä¾‹å¦‚ï¼šGPT-4-turbo å¯å¤„ç† 128K tokensï¼Œè¶…å‡ºå³é—å¿˜ã€‚

### é—®é¢˜

- å¯¹è¯å¤ªé•¿ä¼šé—å¿˜å‰æ–‡
- æˆæœ¬ä¸å»¶è¿Ÿéšä¸Šä¸‹æ–‡å¢åŠ è€Œçº¿æ€§ä¸Šå‡
- ç”¨æˆ·ç”»åƒä¸ä¸ªæ€§ä¸¢å¤±

------

## å…­ã€è¡¥é½çŸ­æ¿ï¼šè®°å¿†ç³»ç»Ÿä¸‰å±‚è®¾è®¡

| å±‚çº§         | å­˜å‚¨ç›®æ ‡                   | ç¤ºä¾‹æŠ€æœ¯                | åŠŸèƒ½è¯´æ˜          |
| ------------ | -------------------------- | ----------------------- | ----------------- |
| **çŸ­æœŸè®°å¿†** | å½“å‰ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘å¤šè½®å¯¹è¯ï¼‰ | Context Window          | ç›´æ¥è¾“å…¥æ¨¡å‹      |
| **ä¸­æœŸè®°å¿†** | é‡è¦æ‘˜è¦ã€ä»»åŠ¡çŠ¶æ€         | Summarizer / ç¼©å†™å™¨     | å‹ç¼©å†—ä½™å†å²      |
| **é•¿æœŸè®°å¿†** | ç”¨æˆ·æ¡£æ¡ˆã€çŸ¥è¯†åº“ã€å†å²äº‹ä»¶ | å‘é‡æ•°æ®åº“ / Mem0 / RAG | æ£€ç´¢ + è¡¥å……ä¸Šä¸‹æ–‡ |

### ä¸‰å¤§ç­–ç•¥

1. **å†™å…¥ç­–ç•¥**ï¼šç²¾ç‚¼ä¸Šä¸‹æ–‡ï¼Œæ˜ç¡®system promptè§’è‰²
2. **æ£€ç´¢ç­–ç•¥**ï¼šç”¨EmbeddingæŸ¥æ‰¾æœ€ç›¸å…³å†å²ç‰‡æ®µ
3. **å‹ç¼©ç­–ç•¥**ï¼šå®šæœŸæ€»ç»“å¯¹è¯ï¼Œç”¨æ‘˜è¦æ›¿ä»£å…¨æ–‡

------

## ä¸ƒã€ä¸Šä¸‹æ–‡ç®¡ç†çš„å®æˆ˜å»ºè®®

1. **åˆ†å±‚å­˜å‚¨**ï¼šçŸ­æœŸå¯¹è¯ + é•¿æœŸè®°å¿† + ç”¨æˆ·ç”»åƒ
2. **ä¸»åŠ¨æ€»ç»“**ï¼šå®šæœŸæ€»ç»“å¯¹è¯ï¼Œé™ä½tokenè´Ÿæ‹…
3. **ä¸Šä¸‹æ–‡éš”ç¦»**ï¼šåŒºåˆ†å†å²ã€çŸ¥è¯†åº“ã€å·¥å…·ç»“æœ
4. **åŠ¨æ€æ›´æ–°**ï¼šç»“åˆå‘é‡æ£€ç´¢ + è‡ªåŠ¨æ‘˜è¦æ¡†æ¶ï¼ˆå¦‚Mem0ï¼‰

> ğŸ”§ æ¨èæ¡†æ¶ï¼š[Mem0](https://github.com/mem0ai/mem0) å¯è‡ªåŠ¨ç®¡ç†å¯¹è¯è®°å¿†ï¼Œæ”¯æŒæ€»ç»“ã€æ£€ç´¢ä¸ä¸ªæ€§åŒ–è®°å¿†æ›´æ–°ã€‚

------

## å…«ã€æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©

åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ›´å¥½ï¼Œä¸èƒ½åªé æ„Ÿè§‰ï¼Œä¸šç•Œé€šè¡Œåšæ³•åŒ…æ‹¬ï¼š

| æ–¹æ³•             | è¯´æ˜                       | å·¥å…·/æŒ‡æ ‡            |
| ---------------- | -------------------------- | -------------------- |
| **å­¦æœ¯åŸºå‡†**     | MMLUã€GSM8Kç­‰æ ‡å‡†æµ‹è¯•      | å¯¹æ¯”ç»¼åˆèƒ½åŠ›         |
| **äººå·¥è¯„ä¼°**     | äººç±»æ ‡æ³¨ä¸»è§‚è´¨é‡ï¼ˆæœ€é‡è¦ï¼‰ | GPT Judge / äººå·¥è¯„åˆ† |
| **A/Bæµ‹è¯•**      | åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹å¯¹æ¯”ç»“æœ   | RAGå®éªŒæ¡†æ¶          |
| **æˆæœ¬æ•ˆç›Šåˆ†æ** | æ€§èƒ½ã€ä»·æ ¼ã€å»¶è¿Ÿç»¼åˆå¹³è¡¡   | tokens/å“åº”æ—¶é—´      |

------

## ä¹ã€å‚æ•°è°ƒä¼˜å®æˆ˜æ‰‹å†Œ

| ä»»åŠ¡ç±»å‹                | Temperature | Top_p   | æ¨èç­–ç•¥     |
| ----------------------- | ----------- | ------- | ------------ |
| **äº‹å®é—®ç­” / ä»£ç ç”Ÿæˆ** | 0.1-0.5     | 0.9-1.0 | ç¨³å®šæ€§ä¼˜å…ˆ   |
| **æ€»ç»“ / ç¿»è¯‘**         | 0.5-0.7     | 0.8-0.9 | å¹³è¡¡å¤šæ ·æ€§   |
| **åˆ›æ„å†™ä½œ / å¯¹è¯**     | 0.8-1.2     | 0.7-0.9 | å¢å¼ºçµæ„Ÿæ„Ÿ   |
| **èŠå¤©æœºå™¨äºº**          | 0.8-1.0     | 0.9     | ä¿æŒè‡ªç„¶æµç•… |

> ğŸ’¡ å®è·µå»ºè®®ï¼š
>
> 1. ä»ä¿å®ˆå‚æ•°å¼€å§‹ï¼Œé€æ­¥æ”¾å®½ temperatureã€‚
> 2. å¯¹æ¯ç§ä»»åŠ¡æ„å»ºå°æ ·æœ¬é›†ï¼ˆ10ï½50ä¸ªpromptï¼‰è¿›è¡Œå‚æ•°æ‰«æã€‚
> 3. åˆ©ç”¨ A/B æµ‹è¯• + äººå·¥è¯„ä¼°ï¼Œå»ºç«‹å¯é‡åŒ–è°ƒä¼˜æœºåˆ¶ã€‚

------

## åã€å®æˆ˜å»ºè®®æ±‡æ€»

âœ… **æ¨¡å‹é€‰æ‹©**
 å…ˆæµ‹å†é€‰ï¼Œä¸ç›²ä¿¡æ’è¡Œæ¦œï¼›å…¼é¡¾å»¶è¿Ÿä¸æˆæœ¬ã€‚

âœ… **å‚æ•°ä¼˜åŒ–**
 temperature â†’ top_p â†’ top_k é¡ºåºè°ƒä¼˜ã€‚

âœ… **ä¸Šä¸‹æ–‡ç­–ç•¥**
 çŸ­æœŸï¼šä¸Šä¸‹æ–‡æ‹¼æ¥ï¼›é•¿æœŸï¼šå¤–éƒ¨è®°å¿†æ£€ç´¢ï¼›åŠ¨æ€ï¼šæ‘˜è¦+æ£€ç´¢åŒç®¡é½ä¸‹ã€‚

âœ… **è¯„ä¼°ä½“ç³»**
 å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•ä¸äººå·¥æ‰“åˆ†é—­ç¯ã€‚

------

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [OpenAI API å‚æ•°æ‰‹å†Œ](https://platform.openai.com/docs/guides/text-generation)
- [Anthropic Claude Prompt Guide](https://docs.anthropic.com/)
- [Mem0: AIè®°å¿†ç®¡ç†æ¡†æ¶](https://github.com/mem0ai/mem0)
- LangChain Cookbook: Context & Memory
- [Andrej Karpathy: Let's Build GPT](https://karpathy.ai/zero-to-hero.html)

---

## åä¸€ã€å‚è€ƒæ–‡çŒ®

### ç›¸å…³è®ºæ–‡

1. **Yao, L., et al. (2022).** "ReAct: Synergizing Reasoning and Acting in Language Models." *arXiv preprint arXiv:2210.03629*. [é“¾æ¥](https://arxiv.org/abs/2210.03629)

2. **Wei, J., et al. (2022).** "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models." *Advances in Neural Information Processing Systems*, 35. [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)

### å·¥å…·ä¸æ¡†æ¶

- [Mem0: AIè®°å¿†ç®¡ç†æ¡†æ¶](https://github.com/mem0ai/mem0) - è‡ªåŠ¨åŒ–å¯¹è¯è®°å¿†ç®¡ç†
- [OpenAI API Documentation](https://platform.openai.com/docs/guides/text-generation) - æ–‡æœ¬ç”Ÿæˆå‚æ•°è¯´æ˜
- [Anthropic Claude Documentation](https://docs.anthropic.com/) - Claudeæ¨¡å‹ä½¿ç”¨æŒ‡å—

### å­¦ä¹ èµ„æº

- Karpathy, A. (2023). "Let's Build GPT." [karpathy.ai](https://karpathy.ai/zero-to-hero.html)
- LangChain Cookbook: [Context & Memory](https://python.langchain.com/docs/use_cases/memory/)

------

## âœ… å°ç»“

> LLMè¿›é˜¶çš„æœ¬è´¨æ˜¯â€”â€”
>  â€œç†è§£å‚æ•°ã€æŒæ§ä¸Šä¸‹æ–‡ã€ç®¡ç†è®°å¿†â€ã€‚
>
> å½“ä½ èƒ½æµç•…è°ƒä¼˜temperatureã€top_pï¼Œå¹¶é©¾é©­ä¸Šä¸‹æ–‡è®°å¿†ç³»ç»Ÿï¼Œä½ å°±çœŸæ­£è·¨è¿‡äº†LLMå¼€å‘çš„é—¨æ§›ã€‚