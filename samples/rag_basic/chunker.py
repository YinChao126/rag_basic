# chunker.py
"""
ç®€å•çš„ Chunkerï¼šæŠŠçº¯æ–‡æœ¬åˆ‡æˆå®šé•¿ç‰‡æ®µå¹¶å†™å…¥ chunks.json
ç”¨æ³•ç¤ºä¾‹ï¼š
    python chunker.py

è¾“å‡ºï¼š
  - chunks.json: åˆ‡åˆ†åŽçš„ [{ "id": int, "source": filename, "text": "..."}]
è¯´æ˜Žï¼š
  - å¼ºåˆ¶ä»¥ UTF-8 è¯»å–æ–‡æœ¬ï¼Œé¿å… GBK è§£ç é”™è¯¯ï¼ˆè¯·ç¡®ä¿æ–‡ä»¶ä¸º UTF-8ï¼‰
"""

import argparse
import json
import os
from pathlib import Path
from typing import List
from extractor import extract_pdf
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def simple_chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    if chunk_size <= 0:
        raise ValueError("chunk_size must be > 0")
    if overlap >= chunk_size:
        raise ValueError("overlap must be < chunk_size")
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + chunk_size, L)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        # move start forward with overlap
        start = end - overlap
        if start < 0:
            start = 0
        if (start >= L) or (end == L):
            break
    return chunks

def langchain_chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """
    ä½¿ç”¨ LangChain çš„ RecursiveCharacterTextSplitter å®žçŽ°æ–‡æœ¬åˆ†å—
    
    Args:
        text (str): éœ€è¦åˆ†å—çš„æ–‡æœ¬
        chunk_size (int): æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦
        overlap (int): å—ä¹‹é—´çš„é‡å é•¿åº¦
        
    Returns:
        List[str]: åˆ†å—åŽçš„æ–‡æœ¬åˆ—è¡¨
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""],
    )
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸º Document å¯¹è±¡è¿›è¡Œåˆ†å‰²
    documents = [Document(page_content=text)]
    splitted_docs = splitter.split_documents(documents)
    
    # æå–åˆ†å‰²åŽçš„æ–‡æœ¬å†…å®¹
    chunks = [doc.page_content.strip() for doc in splitted_docs]
    # è¿‡æ»¤æŽ‰ç©ºå­—ç¬¦ä¸²
    chunks = [chunk for chunk in chunks if chunk]
    
    return chunks

def main():
    import os
    from pathlib import Path
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    # é…ç½®å‚æ•°
    chunk_size = 500
    overlap = 50
    
    print("=" * 70)
    print("âœ‚ï¸  æ–‡æœ¬åˆ‡ç‰‡æµ‹è¯•")
    print("=" * 70)
    
    # è¯»å–æå–çš„æ–‡æœ¬
    pdf_content_file = output_dir / "pdf_content.txt"
    if not pdf_content_file.exists():
        pdf_content_file = Path("pdf_content.txt")
    
    if not pdf_content_file.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æå–çš„æ–‡æœ¬æ–‡ä»¶")
        print("ðŸ’¡ æç¤ºï¼šè¯·å…ˆè¿è¡Œ extractor.py æå–PDFæ–‡æœ¬")
        exit(1)
    
    print(f"\nðŸ“– æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶...")
    print(f"   æ–‡ä»¶è·¯å¾„: {pdf_content_file}")
    
    with open(pdf_content_file, 'r', encoding='utf-8') as f:
        txt = f.read()
    
    print(f"âœ… æ–‡æœ¬è¯»å–æˆåŠŸ")
    print(f"   - æ–‡æœ¬é•¿åº¦: {len(txt)} å­—ç¬¦")
    print(f"   - åˆ‡ç‰‡å‚æ•°: chunk_size={chunk_size}, overlap={overlap}")
    
    # ========== æ–¹æ³•1ï¼šç®€å•åˆ‡ç‰‡ ==========
    print(f"\n{'='*70}")
    print("æ–¹æ³•1ï¼šç®€å•åˆ‡ç‰‡ï¼ˆå›ºå®šé•¿åº¦åˆ‡åˆ†ï¼‰")
    print(f"{'='*70}")
    print(f"ðŸ“ å¼€å§‹åˆ‡ç‰‡...")
    
    chunks_simple = simple_chunk_text(txt, chunk_size, overlap)
    
    print(f"âœ… åˆ‡ç‰‡å®Œæˆï¼")
    print(f"   - åˆ‡ç‰‡æ•°é‡: {len(chunks_simple)} ä¸ª")
    print(f"   - å¹³å‡é•¿åº¦: {sum(len(c) for c in chunks_simple) / len(chunks_simple):.0f} å­—ç¬¦")
    
    # ä¿å­˜ç»“æžœ
    out_list = []
    for i, c in enumerate(chunks_simple):
        out_list.append({"id": i, "source": "test_user_manual.pdf", "text": c})
    
    output_file = output_dir / "chunks_simple.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(out_list, f, ensure_ascii=False, indent=2)
    
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ˜¾ç¤ºç¤ºä¾‹ç‰‡æ®µ
    if chunks_simple:
        print(f"\nðŸ“„ ç¤ºä¾‹ç‰‡æ®µï¼ˆç¬¬1ä¸ªï¼Œå‰150å­—ç¬¦ï¼‰:")
        print("-" * 70)
        print(chunks_simple[0][:150] + "..." if len(chunks_simple[0]) > 150 else chunks_simple[0])
        print("-" * 70)
    
    # ========== æ–¹æ³•2ï¼šLangChainæ™ºèƒ½åˆ‡ç‰‡ ==========
    print(f"\n{'='*70}")
    print("æ–¹æ³•2ï¼šLangChainæ™ºèƒ½åˆ‡ç‰‡ï¼ˆæŒ‰åˆ†éš”ç¬¦åˆ‡åˆ†ï¼‰")
    print(f"{'='*70}")
    print(f"ðŸ“ å¼€å§‹åˆ‡ç‰‡...")
    
    chunks_langchain = langchain_chunk_text(txt, chunk_size, overlap)
    
    print(f"âœ… åˆ‡ç‰‡å®Œæˆï¼")
    print(f"   - åˆ‡ç‰‡æ•°é‡: {len(chunks_langchain)} ä¸ª")
    print(f"   - å¹³å‡é•¿åº¦: {sum(len(c) for c in chunks_langchain) / len(chunks_langchain):.0f} å­—ç¬¦")
    
    # ä¿å­˜ç»“æžœ
    out_list = []
    for i, c in enumerate(chunks_langchain):
        out_list.append({"id": i, "source": "test_user_manual.pdf", "text": c})
    
    output_file = output_dir / "chunks.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(out_list, f, ensure_ascii=False, indent=2)
    
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ˜¾ç¤ºç¤ºä¾‹ç‰‡æ®µ
    if chunks_langchain:
        print(f"\nðŸ“„ ç¤ºä¾‹ç‰‡æ®µï¼ˆç¬¬1ä¸ªï¼Œå‰150å­—ç¬¦ï¼‰:")
        print("-" * 70)
        print(chunks_langchain[0][:150] + "..." if len(chunks_langchain[0]) > 150 else chunks_langchain[0])
        print("-" * 70)
    
    # ========== å¯¹æ¯”æ€»ç»“ ==========
    print(f"\n{'='*70}")
    print("ðŸ“Š åˆ‡ç‰‡æ–¹æ³•å¯¹æ¯”")
    print(f"{'='*70}")
    print(f"{'æ–¹æ³•':<30} {'åˆ‡ç‰‡æ•°é‡':<15} {'å¹³å‡é•¿åº¦':<15}")
    print("-" * 70)
    avg_simple = sum(len(c) for c in chunks_simple) / len(chunks_simple) if chunks_simple else 0
    avg_langchain = sum(len(c) for c in chunks_langchain) / len(chunks_langchain) if chunks_langchain else 0
    print(f"{'ç®€å•åˆ‡ç‰‡':<30} {len(chunks_simple):<15} {avg_simple:.0f}")
    print(f"{'LangChainåˆ‡ç‰‡':<30} {len(chunks_langchain):<15} {avg_langchain:.0f}")
    print(f"{'='*70}")
    
    print(f"\nðŸ’¡ æç¤ºï¼š")
    print(f"   - ç®€å•åˆ‡ç‰‡ï¼šå›ºå®šé•¿åº¦åˆ‡åˆ†ï¼Œå¯èƒ½åˆ‡æ–­è¯­ä¹‰")
    print(f"   - LangChainåˆ‡ç‰‡ï¼šæŒ‰åˆ†éš”ç¬¦æ™ºèƒ½åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§")
    print(f"   - å»ºè®®ä½¿ç”¨ LangChainåˆ‡ç‰‡ï¼ˆchunks.jsonï¼‰è¿›è¡ŒåŽç»­å¤„ç†")

if __name__ == "__main__":
    main()
