# RAG教程术语表（Glossary）

本文档整理了RAG教程中涉及的核心术语，方便快速查阅和理解。

---

## A

### Agent（智能体）
能够感知环境、做出决策并执行动作的AI系统。在RAG场景中，Agent可以自主选择检索策略、调用工具、生成回答。

### Attention（注意力机制）
Transformer架构的核心机制，让模型在处理序列时能够关注到最相关的部分。注意力机制使得模型能够理解长文本的语义关系。

### Alignment（对齐）
通过人类反馈（RLHF）等技术，调整模型输出以符合人类价值观和期望的过程。

---

## B

### BM25
一种经典的关键词检索算法，基于词频和逆文档频率计算相关性。在RAG中常与向量检索结合形成混合检索。

### Batch Size（批次大小）
向量化处理时一次性处理的文本片段数量。较大的batch size可以提高处理效率，但需要更多内存。

---

## C

### Chunk（切片/分块）
将长文档切分成较小的文本片段，便于向量化和检索。切片策略直接影响RAG系统的检索质量。

### Chroma
一个轻量级的开源向量数据库，常用于RAG系统的向量存储和检索。

### Context Window（上下文窗口）
模型一次能够处理的最大token数量。超出上下文窗口的内容会被截断或遗忘。

### Chain-of-Thought (CoT)（思维链）
让模型在生成答案前先展示推理过程的技术，可以提高复杂问题的回答质量。

### Cross-Encoder（交叉编码器）
一种重排序模型，同时编码查询和文档，计算两者的相关性分数。相比向量检索更准确但计算成本更高。

---

## D

### Dashscope
阿里云的大模型服务平台，提供通义千问（Qwen）系列模型和Embedding服务。

### Dense Retrieval（密集检索）
使用向量（Embedding）进行语义检索的方法，相比关键词检索能更好地理解语义。

---

## E

### Embedding（向量化/嵌入）
将文本转换为固定长度的数值向量，用于表示文本的语义信息。向量化是RAG系统实现语义检索的基础。

### Extractor（提取器）
从各种格式文档（PDF、Word、Markdown等）中提取文本内容的组件。

---

## F

### FAISS
Facebook开源的向量相似度搜索库，支持大规模向量检索。

### Fine-tuning（微调）
在预训练模型基础上，使用特定任务的数据进行进一步训练，使模型适应特定应用场景。

### Few-shot（少样本学习）
在Prompt中提供少量示例，引导模型学习任务模式。

---

## G

### Generation（生成）
RAG流程的最后一步，LLM基于检索到的上下文生成最终答案。

### GPT（Generative Pre-trained Transformer）
OpenAI开发的大语言模型系列，包括GPT-3、GPT-4等。

---

## H

### Hallucination（幻觉）
模型生成看似合理但实际错误或不存在的信息。RAG通过检索真实文档来减少幻觉。

### Hybrid Search（混合检索）
结合关键词检索（BM25）和向量检索（Embedding）的检索策略，通常能获得更好的检索效果。

---

## I

### Inference（推理）
使用训练好的模型对新数据进行预测的过程。在RAG中，推理指LLM生成答案的过程。

---

## K

### Knowledge Base（知识库）
RAG系统中存储文档和向量的数据库，是检索的数据源。

---

## L

### LangChain
一个流行的LLM应用开发框架，提供了RAG、Agent、工具调用等功能的完整实现。

### LLM（Large Language Model，大语言模型）
参数量巨大的语言模型，如GPT、Claude、Qwen等，能够理解和生成自然语言。

### LlamaIndex
专注于数据索引和检索的RAG框架，在文档处理方面有独特优势。

---

## M

### Max Tokens（最大Token数）
模型生成答案时的最大长度限制，用于控制成本和输出长度。

### Memory（记忆）
RAG系统中管理对话历史和用户信息的机制，包括短期记忆（上下文）和长期记忆（向量库）。

### Metadata（元数据）
存储文档的附加信息，如来源文件、创建时间、文档类型等，用于检索结果的追溯和过滤。

### Milvus
一个生产级的开源向量数据库，支持大规模向量存储和检索。

---

## O

### Ollama
一个本地大模型运行工具，可以在本地部署和运行开源大模型。

### One-shot（单样本学习）
在Prompt中提供一个示例，引导模型学习任务格式。

### Overlap（重叠）
文本切片时相邻片段之间的重叠部分，用于保持语义完整性。

---

## P

### Prompt（提示词）
输入给LLM的指令和上下文，用于引导模型生成期望的输出。

### Prompt Engineering（提示工程）
设计和优化Prompt的技术，是提升LLM输出质量的关键方法。

### Pre-training（预训练）
在大规模无标注文本上训练模型，学习语言的基本规律。

---

## R

### RAG（Retrieval-Augmented Generation，检索增强生成）
结合检索和生成的技术，通过检索外部知识库来增强LLM的回答能力。

### Rerank（重排序）
对初步检索结果进行重新排序，提升最相关文档的排名。

### Retrieval（检索）
根据用户问题，从知识库中找到最相关的文档片段。

### ReAct（Reasoning + Acting）
结合推理和行动的Agent机制，让模型能够思考并调用工具执行任务。

### RecursiveCharacterTextSplitter
LangChain提供的递归文本分割器，能够根据分隔符智能切分文本。

---

## S

### Semantic Search（语义检索）
基于语义相似度而非关键词匹配的检索方法，通过向量化实现。

### Split（分割/切片）
将长文档切分成小块的过程，是RAG流程的第一步。

### Streaming（流式输出）
实时输出模型生成的内容，而不是等待全部生成完成，提升用户体验。

### System Prompt（系统提示词）
定义模型角色和行为的基础Prompt，影响模型的整体回答风格。

---

## T

### Temperature（温度参数）
控制模型输出随机性的参数。值越低输出越确定，值越高输出越有创造性。

### Token
模型处理的最小单位，可能是一个字、词或符号。不同模型的token化方式不同。

### Top-k
采样时只考虑概率最高的k个候选词，用于控制输出的多样性。

### Top-p（Nucleus Sampling）
采样时保留累计概率达到p的候选词集合，动态控制候选词数量。

### Transformer
Google在2017年提出的神经网络架构，是现代LLM的基础架构。

---

## V

### Vector Database（向量数据库）
专门用于存储和检索向量的数据库，如Chroma、FAISS、Milvus等。

### Vector Store（向量存储）
RAG系统中存储文档向量的组件，提供向量检索功能。

---

## Z

### Zero-shot（零样本学习）
模型在没有示例的情况下直接完成任务，完全依赖自身理解能力。

---

## 相关概念

### 生产者-消费者模型
RAG系统可以理解为两个过程：
- **知识生产**：文档 → 切片 → 向量化 → 存储
- **知识消费**：问题 → 检索 → 生成答案

### 人工+智能
RAG系统的成功离不开人工参与：文档整理、切片策略设计、参数调优等都需要人工经验。

---

## 参考资源

- [LangChain术语表](https://python.langchain.com/docs/glossary)
- [AI术语词典](https://www.aisummarizer.com/glossary)



